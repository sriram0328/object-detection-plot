{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOdKpoL3reNZ8oEV/GbzOvq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8f537012b7f84eb18b0cf3f4f1d13155":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a49a5e277cd3435aa7abcc581d55f693","IPY_MODEL_f7004ef63eac4d1c95526585eb544e68","IPY_MODEL_00032a8811ad40bfb2d01dae47e558ca"],"layout":"IPY_MODEL_dfebdd95fa564d16bdf0410d930aa0fc"}},"a49a5e277cd3435aa7abcc581d55f693":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5daf26cc144b442db118d87c5e51cbdb","placeholder":"​","style":"IPY_MODEL_20fc1f8977704c9995d8599547e971e6","value":"100%"}},"f7004ef63eac4d1c95526585eb544e68":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_322b668647814f45af1aa69aa32d9a4b","max":335,"min":0,"orientation":"horizontal","style":"IPY_MODEL_70d4f9ed14604322b9a2a6cc3db1750b","value":335}},"00032a8811ad40bfb2d01dae47e558ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca2e2d77b5934f2f91160d53ee386e61","placeholder":"​","style":"IPY_MODEL_7fba2bd7fe134b14816cd8f3fab64041","value":" 335/335 [18:59&lt;00:00,  3.31s/it]"}},"dfebdd95fa564d16bdf0410d930aa0fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5daf26cc144b442db118d87c5e51cbdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20fc1f8977704c9995d8599547e971e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"322b668647814f45af1aa69aa32d9a4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70d4f9ed14604322b9a2a6cc3db1750b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ca2e2d77b5934f2f91160d53ee386e61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fba2bd7fe134b14816cd8f3fab64041":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8f537012b7f84eb18b0cf3f4f1d13155","a49a5e277cd3435aa7abcc581d55f693","f7004ef63eac4d1c95526585eb544e68","00032a8811ad40bfb2d01dae47e558ca","dfebdd95fa564d16bdf0410d930aa0fc","5daf26cc144b442db118d87c5e51cbdb","20fc1f8977704c9995d8599547e971e6","322b668647814f45af1aa69aa32d9a4b","70d4f9ed14604322b9a2a6cc3db1750b","ca2e2d77b5934f2f91160d53ee386e61","7fba2bd7fe134b14816cd8f3fab64041"],"output_embedded_package_id":"1s4bO7mGHNVmHcUfmOgtk5EX30lHcCTmE"},"id":"lNT8k9OExNA1","executionInfo":{"status":"ok","timestamp":1724338104275,"user_tz":-330,"elapsed":1156611,"user":{"displayName":"Sriram Ramanadham","userId":"16457127170613387931"}},"outputId":"a4f16ed3-9c71-4aaa-d5dc-2466d1f574b1"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["!pip install ultralytics -q\n","# Object Detecion\n","import cv2\n","from ultralytics import YOLO\n","#plots\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#basics\n","import pandas as pd\n","import numpy as np\n","import os\n","import subprocess\n","\n","from tqdm.notebook import tqdm\n","\n","# Display image and videos\n","import IPython\n","from IPython.display import Video, display\n","%matplotlib inline\n","# Video  path for experiment\n","path = \"/content/traffic jam.mp4\"\n","# Displaying the target video\n","frac = 0.65\n","display(Video(data=path, height=int(720*frac), width=int(1280*frac), embed = True))\n","#loading a YOLO model\n","model = YOLO('yolov8x.pt')\n","#geting names from classes\n","dict_classes = model.model.names\n","# Auxiliary functions\n","def risize_frame(frame, scale_percent):\n","    \"\"\"Function to resize an image in a percent scale\"\"\"\n","    width = int(frame.shape[1] * scale_percent / 100)\n","    height = int(frame.shape[0] * scale_percent / 100)\n","    dim = (width, height)\n","\n","    # resize image\n","    resized = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n","    return resized\n","\n","### Configurations\n","#Verbose during prediction\n","verbose = False\n","# Scaling percentage of original frame\n","scale_percent = 50\n","\n","# Reading video with cv2\n","video = cv2.VideoCapture(path)\n","\n","# Objects to detect Yolo\n","class_IDS = [2, 3, 5, 7]\n","\n","# Auxiliary variables\n","centers_old = {}\n","centers_new = {}\n","obj_id = 0\n","veiculos_contador_in = dict.fromkeys(class_IDS, 0)\n","veiculos_contador_out = dict.fromkeys(class_IDS, 0)\n","end = []\n","frames_list = []\n","cy_linha = int(1500 * scale_percent/100 )\n","cx_sentido = int(2000 * scale_percent/100)\n","offset = int(8 * scale_percent/100 )\n","contador_in = 0\n","contador_out = 0\n","print(f'[INFO] - Verbose during Prediction: {verbose}')\n","\n","# Original informations of video\n","height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n","fps = video.get(cv2.CAP_PROP_FPS)\n","print('[INFO] - Original Dim: ', (width, height))\n","\n","# Scaling Video for better performance\n","if scale_percent != 100:\n","    print('[INFO] - Scaling change may cause errors in pixels lines ')\n","    width = int(width * scale_percent / 100)\n","    height = int(height * scale_percent / 100)\n","    print('[INFO] - Dim Scaled: ', (width, height))\n","\n","### Video output ####\n","video_name = 'traffic jam.mp4'\n","output_path = \"rep_\" + video_name\n","tmp_output_path = \"tmp_\" + output_path\n","VIDEO_CODEC = \"MP4V\"\n","\n","output_video = cv2.VideoWriter(tmp_output_path,\n","                               cv2.VideoWriter_fourcc(*VIDEO_CODEC),\n","                               fps, (width, height))\n","\n","# Executing Recognition\n","for i in tqdm(range(int(video.get(cv2.CAP_PROP_FRAME_COUNT)))):\n","\n","    # reading frame from video\n","    ret, frame = video.read()\n","\n","    # Check if a frame was successfully read\n","    if not ret:\n","        print(\"Error reading frame. Skipping...\")\n","        continue  # Skip to the next iteration\n","\n","    # Applying resizing of read frame\n","    frame = risize_frame(frame, scale_percent)\n","\n","    if verbose:\n","        print('Dimension Scaled(frame): ', (frame.shape[1], frame.shape[0]))\n","\n","    # Getting predictions\n","    y_hat = model.predict(frame, conf=0.7, classes=class_IDS, device='cpu', verbose=False)\n","\n","    # Getting the bounding boxes, confidence and classes of the recognize objects in the current frame.\n","    boxes = y_hat[0].boxes.xyxy.cpu().numpy()\n","    conf = y_hat[0].boxes.conf.cpu().numpy()\n","    classes = y_hat[0].boxes.cls.cpu().numpy()\n","\n","    # Storing the above information in a dataframe\n","    positions_frame = pd.DataFrame({\n","        'xmin': boxes[:, 0],\n","        'ymin': boxes[:, 1],\n","        'xmax': boxes[:, 2],\n","        'ymax': boxes[:, 3],\n","        'conf': conf,\n","        'class': classes,\n","    })\n","\n","    #Translating the numeric class labels to text\n","    labels = [dict_classes[i] for i in classes]\n","\n","    # Drawing transition line for in\\out vehicles counting\n","    cv2.line(frame, (0, cy_linha), (int(4500 * scale_percent/100 ), cy_linha), (255,255,0),8)\n","\n","    # For each vehicles, draw the bounding-box and counting each one the pass thought the transition line (in\\out)\n","    for ix, row in enumerate(positions_frame.iterrows()):\n","        # Getting the coordinates of each vehicle (row)\n","        xmin, ymin, xmax, ymax, confidence, category,  = row[1].astype('int')\n","\n","        # Calculating the center of the bounding-box\n","        center_x, center_y = int(((xmax+xmin))/2), int((ymax+ ymin)/2)\n","\n","        # drawing center and bounding-box of vehicle in the given frame\n","        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (255,0,0), 5) # box\n","        cv2.circle(frame, (center_x,center_y), 5,(255,0,0),-1) # center of box\n","\n","        #Drawing above the bounding-box the name of class recognized.\n","        cv2.putText(img=frame, text=labels[ix]+' - '+str(np.round(conf[ix],2)),\n","                    org= (xmin,ymin-10), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 0, 0),thickness=2)\n","\n","        # Checking if the center of recognized vehicle is in the area given by the transition line + offset and transition line - offset\n","        if (center_y < (cy_linha + offset)) and (center_y > (cy_linha - offset)):\n","            if  (center_x >= 0) and (center_x <=cx_sentido):\n","                contador_in +=1\n","                veiculos_contador_in[category] += 1\n","            else:\n","                contador_out += 1\n","                veiculos_contador_out[category] += 1\n","\n","    #updating the counting type of vehicle\n","    contador_in_plt = [f'{dict_classes[k]}: {i}' for k, i in veiculos_contador_in.items()]\n","    contador_out_plt = [f'{dict_classes[k]}: {i}' for k, i in veiculos_contador_out.items()]\n","\n","    #drawing the number of vehicles in\\out\n","    cv2.putText(img=frame, text='N. vehicles In',\n","                org= (30,30), fontFace=cv2.FONT_HERSHEY_TRIPLEX,\n","                fontScale=1, color=(255, 255, 0),thickness=1)\n","\n","    cv2.putText(img=frame, text='N. vehicles Out',\n","                org= (int(2800 * scale_percent/100 ),30),\n","                fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 255, 0),thickness=1)\n","\n","    #drawing the counting of type of vehicles in the corners of frame\n","    xt = 40\n","    for txt in range(len(contador_in_plt)):\n","        xt +=30\n","        cv2.putText(img=frame, text=contador_in_plt[txt],\n","                    org= (30,xt), fontFace=cv2.FONT_HERSHEY_TRIPLEX,\n","                    fontScale=1, color=(255, 255, 0),thickness=1)\n","\n","        cv2.putText(img=frame, text=contador_out_plt[txt],\n","                    org= (int(2800 * scale_percent/100 ),xt), fontFace=cv2.FONT_HERSHEY_TRIPLEX,\n","                    fontScale=1, color=(255, 255, 0),thickness=1)\n","\n","    #drawing the number of vehicles in\\out\n","    cv2.putText(img=frame, text=f'In:{contador_in}',\n","                org= (int(1820 * scale_percent/100 ),cy_linha+60),\n","                fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 255, 0),thickness=2)\n","\n","    cv2.putText(img=frame, text=f'Out:{contador_out}',\n","                org= (int(1800 * scale_percent/100 ),cy_linha-40),\n","                fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=1, color=(255, 255, 0),thickness=2)\n","\n","    if verbose:\n","        print(contador_in, contador_out)\n","    #Saving frames in a list\n","    frames_list.append(frame)\n","    #saving transformed frames in a output video formaat\n","    output_video.write(frame)\n","\n","#Releasing the video\n","output_video.release()\n","####  pos processing\n","# Fixing video output codec to run in the notebook\\browser\n","if os.path.exists(output_path):\n","    os.remove(output_path)\n","\n","subprocess.run([\"ffmpeg\",  \"-i\", tmp_output_path,\"-crf\",\"18\",\"-preset\",\"veryfast\",\"-hide_banner\",\"-loglevel\",\"error\",\"-vcodec\",\"libx264\",output_path])\n","os.remove(tmp_output_path)\n","\n","# Checking samples of processed frames\n","for i in [28, 29, 32, 40, 42, 50, 58]:\n","    plt.figure(figsize =( 14, 10))\n","    plt.imshow(frames_list[i])\n","    plt.show()"]}]}